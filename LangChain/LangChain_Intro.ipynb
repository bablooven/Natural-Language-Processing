{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8O82Lz--NdJ"
      },
      "source": [
        "#LangChain: Intro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8kMC7jLDKbc"
      },
      "outputs": [],
      "source": [
        "# Adopted from the article: https://www.datacamp.com/tutorial/how-to-build-llm-applications-with-langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwNLOSYn-LrC"
      },
      "source": [
        "### Introduction to LangChain\n",
        "LangChain is an open-source framework designed to facilitate the development of applications powered by large language models (LLMs). It offers a suite of tools, components, and interfaces that simplify the construction of LLM-centric applications. With LangChain, it becomes effortless to manage interactions with language models, seamlessly link different components, and incorporate resources such as APIs and databases. You can read more about LangChain For Data Engineering and Data Applications in a separate article.\n",
        "\n",
        "The LangChain platform comes with a collection of APIs that developers can embed in their applications, empowering them to infuse language processing capabilities without having to build everything from the ground up. Therefore, LangChain efficiently simplifies the process of crafting LLM-based applications, making it suitable for developers across the spectrum of expertise.\n",
        "\n",
        "Applications like chatbots, virtual assistants, language translation utilities, and sentiment analysis tools are all instances of LLM-powered apps. Developers leverage LangChain to create bespoke language model-based applications that cater to specific needs.\n",
        "\n",
        "With the continual advancements and broader adoption of natural language processing, the potential applications of this technology are expected to be virtually limitless. Here are several noteworthy characteristics of LangChain:\n",
        "\n",
        "1. Tailorable prompts to meet your specific requirements\n",
        "\n",
        "2. Constructing chain link components for advanced usage scenarios\n",
        "\n",
        "3. Integrating models for data augmentation and accessing top-notch language model capabilities, such as GPT and HuggingFace Hub.\n",
        "\n",
        "4. Versatile components that allow mixing and matching for specific needs\n",
        "\n",
        "5. Manipulating context to establish and guide context for enhanced precision and user satisfaction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmADjpaz-lJz"
      },
      "source": [
        "### Setting up LangChain in Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1RaL-Et-BFs",
        "outputId": "ec88ca1a-6599-4895-f9f0-7adf1d1f88d1"
      },
      "outputs": [],
      "source": [
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhxbDAat-zfV"
      },
      "source": [
        "By default, the dependencies required for these integrations are NOT included in the installation. To install all dependencies, you can run the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aVc7JSUZ-1Vd",
        "outputId": "81d5232b-909a-45a9-ad58-475b5b7e2a2a"
      },
      "outputs": [],
      "source": [
        "!pip install langchain[all]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49iSyxxlB7PO"
      },
      "source": [
        "### Environment setup\n",
        "Using LangChain usually requires integrations with various model providers, data stores, APIs, and similar components. As with any integration, we must provide appropriate and relevant API keys for LangChain to function. There are two ways to achieve this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "EWlqrN_gB6eV"
      },
      "outputs": [],
      "source": [
        "OPENAI_API_KEY=\"###\"\n",
        "HUGGING_FACE_API_KEY=\"###\" # Anonymized the API keys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbT_6J8VDH0B"
      },
      "source": [
        "LangChain provides an LLM class designed for interfacing with various language model providers, such as OpenAI, Cohere, and Hugging Face. The most basic functionality of an LLM is generating text. It is very straightforward to build an application with LangChain that takes a string prompt and returns the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "8Nhny25NCIvE"
      },
      "outputs": [],
      "source": [
        "# using legacy models\n",
        "from langchain.llms import OpenAI\n",
        "llm = OpenAI(model='text-davinci-003',openai_api_key = OPENAI_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1hFaxA_DYCU",
        "outputId": "bfa3719e-8caa-475a-8ff3-0d58eb91d0d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Q: What did the data scientist say when he found out his data was wrong?\n",
            "A: Oh, samples!\n"
          ]
        }
      ],
      "source": [
        "print(llm(\"Tell me a joke about a data scientist\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxRf-UPDFg-x"
      },
      "source": [
        "In the example above, we are using text-ada-001 model from OpenAI. If you would like to swap that for any open-source models from HuggingFace, itâ€™s a simple change:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNtVgfqOGDrj",
        "outputId": "1a28c99b-125b-471b-c9bb-045eccc87238"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '0.19.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "from langchain import HuggingFaceHub\n",
        "llm = HuggingFaceHub(repo_id = \"google/flan-t5-xl\", huggingfacehub_api_token = HUGGING_FACE_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jv6alRpAGLoj"
      },
      "outputs": [],
      "source": [
        "print(llm(\"Tell me a joke about a data scientist\")) # Failed to return a response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqXk4V7KGXvF"
      },
      "source": [
        "If you have multiple prompts, you can send a list of prompts at once using the generate method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "FDuOOOhnGV6F"
      },
      "outputs": [],
      "source": [
        "llm_response = llm.generate(['Tell me a joke about data scientist',\n",
        "\n",
        "'Tell me a joke about recruiter',\n",
        "\n",
        "'Tell me a joke about psychologist'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxUBfX5SG6pY",
        "outputId": "d825d6bd-1137-4869-8b28-1b1444ef10a4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Generation(text='\\n\\nQ: How many data scientists does it take to change a light bulb?\\nA: None, they just write a map reduce job and let the cluster figure it out!', generation_info={'finish_reason': 'stop', 'logprobs': None})"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm_response.generations[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfIHEsluHRI1",
        "outputId": "0e9ffb9f-4d62-4b8b-a2e2-f790501dce72"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Generation(text='\\n\\nQ: Why did the recruiter cross the road?\\nA: To find the right candidate!', generation_info={'finish_reason': 'stop', 'logprobs': None})"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm_response.generations[1][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JcFq-X2Hg3e"
      },
      "source": [
        "A PromptTemplate in LangChain allows you to use templating to generate a prompt. This is useful when you want to use the same prompt outline in multiple places but with certain values changed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDlhZVlIHiD4",
        "outputId": "5f25d244-f7b1-438e-ff3e-9ada21159000"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM Output: \n",
            "\n",
            "1. Visit the Sun Temple at Konark: The 13th-century Sun Temple is one of the most beautiful architectural wonders in India, with intricate carvings and sculptures that tell the story of the famous Ramayana.\n",
            "\n",
            "2. Explore the Udayagiri and Khandagiri Caves: These 2nd-century caves are a great place to learn more about the history and culture of Odisha.\n",
            "\n",
            "3. Visit the Chilika Lake: Chilika Lake is the largest coastal lagoon in India and home to many rare species of birds, making it a great spot for bird watching.\n"
          ]
        }
      ],
      "source": [
        "USER_INPUT = 'Odisha'\n",
        "template = \"\"\" I am travelling to {location}. What are the top 3 things I can do while I am there. Be very specific and respond as three bullet points \"\"\"\n",
        "from langchain import PromptTemplate\n",
        "prompt = PromptTemplate(\n",
        "input_variables=[\"location\"],\n",
        "template=template,\n",
        ")\n",
        "\n",
        "final_prompt = prompt.format(location=USER_INPUT )\n",
        "\n",
        "print(f\"LLM Output: {llm(final_prompt)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eL9HDv0FIVPX"
      },
      "source": [
        "### Combining LLMs and Prompts in Multi-Step Workflows\n",
        "Chaining within the LangChain context refers to the act of integrating LLMs with other elements to build an application. Several examples include:\n",
        "\n",
        "*   Sequentially combining multiple LLMs by using the output of the first LLM  \n",
        "as input for the second LLM\n",
        "*   Integrating LLMs with prompt templates\n",
        "*   Merging LLMs with external data, such as for question answering\n",
        "*   Incorporating LLMs with long-term memory, like chat history\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzmIreHEIXJC",
        "outputId": "ab8e8d29-96ec-4a68-e137-e3302a3538bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3m\n",
            "\n",
            "Toronto\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3m\n",
            "\n",
            "1. Visit the CN Tower\n",
            "2. Explore the Distillery District\n",
            "3. Take a stroll along the Harbourfront boardwalk\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import LLMChain, SimpleSequentialChain\n",
        "\n",
        "from langchain import PromptTemplate\n",
        "# first step in chain\n",
        "\n",
        "template = \"What is the most popular city in {country} for tourists? Just return the name of the city\"\n",
        "\n",
        "first_prompt = PromptTemplate(\n",
        "\n",
        "input_variables=[\"country\"],\n",
        "\n",
        "template=template)\n",
        "\n",
        "chain_one = LLMChain(llm = llm, prompt = first_prompt)\n",
        "\n",
        "# second step in chain\n",
        "\n",
        "second_prompt = PromptTemplate(\n",
        "\n",
        "input_variables=[\"city\"],\n",
        "\n",
        "template=\"What are the top three things to do in this: {city} for tourists. Just return the answer as three bullet points.\",)\n",
        "\n",
        "chain_two = LLMChain(llm=llm, prompt=second_prompt)\n",
        "\n",
        "# Combine the first and the second chain\n",
        "\n",
        "overall_chain = SimpleSequentialChain(chains=[chain_one, chain_two], verbose=True)\n",
        "\n",
        "final_answer = overall_chain.run(\"Canada\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
